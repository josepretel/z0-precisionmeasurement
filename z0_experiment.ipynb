{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Optimize lepton selection\n",
    "\n",
    "* First, print the distributions of the relevant variables for *all* the Monte Carlo samples (i.e. all the *channels* of the $Z$-boson decay to be studied). Which variables are these? Give sensible ranges to include all the events in the samples (both MC and OPAL data) \n",
    "* Do the same for **one** of the OPAL data samples (your lab assistant will decide which one you choose).\n",
    "* Describe the results.\n",
    "* Optimize the object selection by applying cuts. Make a strategy on how to proceed to find the optimal selection. which information do you need? in thin the\n",
    "* Determine the efficiency and the amount of background for each $Z$ decay channel. Use the simulated events $e^+e^-$, $\\mu^+\\mu^-$, $\\tau^+\\tau^-$ and hadrons ($qq$). Represent the result in a matrix form and think carefully about how you have to correct the measured rates. Don't forget to calculate the errors!\n",
    "* How do we estimate the statistical fluctuations per bin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download missing libraries\n",
    "Comment in the following two lines in case some of the libraries cannot be imported. Please restart the kernel after download+upgrade has successfully finished.\n",
    "\n",
    "**Please comment in these lines when the libraries cannot be imported below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download libraries\n",
    "#%pip install uproot \n",
    "#%pip install awkward \n",
    "#%pip install mplhep \n",
    "#%pip install numpy \n",
    "#%pip install matplotlib \n",
    "#%pip install scipy\n",
    "\n",
    "### Upgrade libraries to latest version\n",
    "#%pip install uproot awkward mplhep numpy matplotlib scipy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import mplhep\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Example*: Reading a ROOT.TTree and plotting a variable with a cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will open data and Monte Carlo samples using **uproot**. Uproot is a reader and a writer of the ROOT file format using only Python and Numpy. Unlike PyROOT and root_numpy, uproot does not depend on C++ ROOT so that no local compilation of the ROOT libraries is needed to access the data.\n",
    "\n",
    "You can find more info on uproot following the references:\n",
    "* Github repo: https://github.com/scikit-hep/uproot4\n",
    "* Tutorial: https://masonproffitt.github.io/uproot-tutorial/\n",
    "* Video tutorial on uproot and awkward arrays:  https://www.youtube.com/embed/ea-zYLQBS4U \n",
    "\n",
    "First, let's specify the folder path for both data and Monte Carlo (MC) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'samples/data/'\n",
    "#path_mc = 'samples/data'\n",
    "\n",
    "### Open the file introducing file path\n",
    "file = uproot.open(path_data+'daten.root')\n",
    "ttree_name = 'myTTree'\n",
    "\n",
    "### Print list of 'branches' of the TTree (i.e. list of variable names)\n",
    "file[ttree_name].keys()\n",
    "\n",
    "## Load branches\n",
    "branches = file[ttree_name].arrays()\n",
    "\n",
    "## Define an numpy array for 'Pcharged'\n",
    "var = 'Pcharged'\n",
    "pchar = ak.to_numpy(branches[var]) # See Docu (https://awkward-array.org/how-to-convert-numpy.html) for more conversions\n",
    "\n",
    "print(f\"Array of type '{type(pchar)}' defined for '{var}':\\n{pchar}\")\n",
    "print(pchar.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line shows all the variables available in the TTree to carry out the experiment. The meaning of these is described in the following table\n",
    "\n",
    "| Variable name | Description |\n",
    "| --- | --- | \n",
    "| <pre>run</pre> | Run number |\n",
    "| <pre>event</pre> | Event number |\n",
    "| <pre>Ncharged</pre> | Number of charged tracks |\n",
    "| <pre>Pcharged</pre> | Total scalar sum of track momenta |\n",
    "| <pre>E_ecal</pre> | Total energy measured in the electromagnetic calorimeter |\n",
    "| <pre>E_hcal</pre> | Total energy measured in the hadronic calorimete |\n",
    "| <pre>E_lep</pre> | LEP beam energy (=$\\sqrt{s}/2$) |\n",
    "| <pre>cos_thru</pre> | cosine of the polar angle between beam axis and thrust axis |\n",
    "| <pre>cos_thet</pre> | cosine of the polar angle between incoming positron and outgoing positive particle |\n",
    "\n",
    "We proceed to plot *PCharged* for illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find all features provided by the function *matplotlib.pyplot.hist()* in the **matplotlib documentation**: https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.hist.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a cut\n",
    "\n",
    "Cuts are applied by *masking* the array. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a mask for certain selection\n",
    "mymask = branches['Pcharged'] >= 18.4\n",
    "#print(mymask)\n",
    "\n",
    "## The sum of this array provides the number of events that passed this cut\n",
    "print(f\"A total of '{sum(mymask)}' out of '{len(mymask)}' events passed the cut 'mymask'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful information about your selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Mean of {var}: ({pchar.mean()}) {np.nanmean(pchar)}\")\n",
    "print(f\"Standard deviation of {var}: ({pchar.std()}) {np.nanstd(pchar)}\")\n",
    "print(f\"Minimum value of {var}: ({pchar.min()}) {np.nanmin(pchar)}\")\n",
    "print(f\"Maximum value of {var}: ({pchar.max()}) {np.nanmax(pchar)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plot with a certain selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(mplhep.style.ATLAS) # You can load ATLAS/CMS/ALICE plot style \n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "bin_content, bin_edges, _ = plt.hist(pchar[mymask],bins=1000,range=(0.,200.), histtype='step',  linewidth=2, edgecolor='b', hatch='/', label='Pcharged')\n",
    "mid = 0.5*(bin_edges[1:] + bin_edges[:-1]) #Calculate midpoint of the bars\n",
    "\n",
    "error_sizes = np.sqrt(bin_content)\n",
    "\n",
    "plt.errorbar(mid, bin_content, yerr=error_sizes, fmt='none')\n",
    "### When producing an histogram, you can store the bin content and the edges of the bins in \n",
    "###'bin_content', and 'bin_edges' \n",
    "#print(bin_content)\n",
    "#print(bin_edges)\n",
    "\n",
    "### Show the plot on screen\n",
    "plt.title('My title')\n",
    "plt.xlim(10.,150.)\n",
    "plt.xlabel('Total scalar sum of track momenta, $p_{track}$')\n",
    "plt.ylabel('Number of events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical uncertainties to the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the statistical uncertainties computed above in 'error_sizes'. \n",
    "* **Are these sensible? Why do we use this formula?** (Hint: Making an histogram is, in short, a *counting experiment*. In the limit of large total number of events, the (binomial) probability function limits to the *Poisson distribution*. What is the variance? And the standard deviation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Example*: How to fit a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "mynewmask = branches['Pcharged'] >= 31.23\n",
    "\n",
    "# Plot data with statistical uncertainties\n",
    "new_bin_content, new_bin_edges, _ = plt.hist(pchar[mynewmask],bins=25,range=(25.,75.),  histtype='step', linewidth=2, label='Pcharged')\n",
    "new_mid = 0.5*(new_bin_edges[1:] + new_bin_edges[:-1]) #Calculate midpoint of the bars\n",
    "new_error_sizes = np.sqrt(new_bin_content)\n",
    "\n",
    "plt.errorbar(new_mid, new_bin_content, yerr=new_error_sizes, fmt='none')\n",
    "\n",
    "### Show the plot on screen\n",
    "plt.title('My title')\n",
    "plt.xlim(30.,80.)\n",
    "plt.xlabel('Total scalar sum of track momenta, $p_{track}$')\n",
    "plt.ylabel('Number of events')\n",
    "\n",
    "\n",
    "# Define model function to be used to fit to the data above:\n",
    "def gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "# p0 is the initial guess for the fitting coefficients (A, mu and sigma above)\n",
    "p0 = [321., 50., 32.]\n",
    "\n",
    "## Fit curve (WARNING: The fit does not propagate bin uncertainties to the uncertainties of the fit parameters!)\n",
    "coeff, var_matrix = curve_fit(gauss, new_mid, new_bin_content, p0=p0)\n",
    "\n",
    "# Get the fitted curve\n",
    "hist_fit = gauss(new_mid, *coeff)\n",
    "\n",
    "plt.plot(new_mid, hist_fit, label='Fit')\n",
    "\n",
    "plt.title(r'$\\mathrm{Histogram\\ of\\ IQ:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(coeff[1], abs(coeff[2])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inversion\n",
    "To determine the uncertainties of the matrix elements after the inversion we use Monte Carlo toy experiments. In this context, what are the advantages and disadvantages of this method when compared to analytical expressions? Discuss it briefly.\n",
    "\n",
    "**References**:\n",
    "* Propagation of Errors for Matrix Inversion: https://arxiv.org/abs/hep-ex/9909031v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[0.934, 0.02,  0.01, 0.],\n",
    "                  [0.01,  0.946, 0.01, 0.01],\n",
    "                  [0.01,  0.01,  0.965,0.01],\n",
    "                  [0.01,  0.01,  0.01, 0.999]])\n",
    "\n",
    "error_matrix = np.array([[0.001,   0.0001,  0.0001, 0.0001],\n",
    "                        [0.0001,  0.001,   0.0001, 0.0001],\n",
    "                        [0.0001,  0.0001,  0.001,  0.0001],\n",
    "                        [0.0001,  0.0001,  0.0001, 0.001]])\n",
    "\n",
    "### Number of toy experiments to be done\n",
    "ntoy = 1000\n",
    "\n",
    "### Create numpy matrix of list to append elements of inverted toy matrices\n",
    "inverse_toys = np.empty((4,4))\n",
    "\n",
    "# Create toy efficiency matrix out of gaussian-distributed random values\n",
    "for i in range(0,ntoy,1):\n",
    "    toy_matrix = np.zeros((4,4))\n",
    "    toy_matrix = np.random.normal(matrix,error_matrix,size=(4,4))\n",
    "    \n",
    "    ### Invert toy matrix\n",
    "    inverse_toy = np.linalg.inv(toy_matrix)\n",
    "    \n",
    "    #print(inverse_toys.item(0,0),inverse_toy.item(0,0))\n",
    "    # Append values\n",
    "    inverse_toys = np.dstack((inverse_toys,inverse_toy))\n",
    "    \n",
    "# Define gaussian function to fit to the toy distributions:\n",
    "def gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "\n",
    "inverse_errors = np.zeros((4,4))\n",
    "inverse_means = np.zeros((4,4))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10),dpi=80)\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "ax00 = plt.subplot(4,4,1)\n",
    "ax01 = plt.subplot(4,4,2)\n",
    "ax02 = plt.subplot(4,4,3)\n",
    "ax03 = plt.subplot(4,4,4)\n",
    "\n",
    "ax10 = plt.subplot(4,4,5)\n",
    "ax11 = plt.subplot(4,4,6)\n",
    "ax12 = plt.subplot(4,4,7)\n",
    "ax13 = plt.subplot(4,4,8)\n",
    "\n",
    "ax20 = plt.subplot(4,4,9)\n",
    "ax21 = plt.subplot(4,4,10)\n",
    "ax22 = plt.subplot(4,4,11)\n",
    "ax23 = plt.subplot(4,4,12)\n",
    "\n",
    "ax30 = plt.subplot(4,4,13)\n",
    "ax31 = plt.subplot(4,4,14)\n",
    "ax32 = plt.subplot(4,4,15)\n",
    "ax33 = plt.subplot(4,4,16)\n",
    "\n",
    "axes = [[ax00,ax01,ax02,ax03],\n",
    "        [ax10,ax11,ax12,ax13],\n",
    "        [ax20,ax21,ax22,ax23],\n",
    "        [ax30,ax31,ax32,ax33]]\n",
    "\n",
    "## Find suitable ranges to fit/plot gaussian distributions\n",
    "ranges = [[(0.9,1.1)   ,(-0.02,0.02), (-0.02,0.02), (-0.02,0.02)],\n",
    "          [(-0.02,0.02),(0.9,1.1)   , (-0.02,0.02), (-0.02,0.02)],\n",
    "          [(-0.02,0.02),(-0.02,0.02), (0.9,1.1)   , (-0.02,0.02)],\n",
    "          [(-0.02,0.02),(-0.02,0.02), (-0.02,0.02), (0.9,1.1)]]\n",
    "\n",
    "\n",
    "# Fill histograms for each inverted matrix coefficient:\n",
    "for j in range(0,4,1):\n",
    "    for k in range(0,4,1):\n",
    "        \n",
    "        # Diagonal and off-diagonal terms have different histogram ranges\n",
    "        hbins, hedges, _ = axes[j][k].hist(inverse_toys[j,k,:],bins=30,range=ranges[j][k],  histtype='step', linewidth=2, label=f'toyhist{j}{k}')\n",
    "        axes[j][k].legend()\n",
    "\n",
    "\n",
    "        # Get the fitted curve\n",
    "        h_mid = 0.5*(hedges[1:] + hedges[:-1]) #Calculate midpoints for the fit\n",
    "        coeffs, _ = curve_fit(gauss, h_mid, hbins, maxfev=10000)\n",
    "        h_fit = gauss(h_mid, *coeffs)\n",
    "        \n",
    "        axes[j][k].plot(h_mid, h_fit,label=f'Fit{j}{k}')\n",
    "\n",
    "        ## To avoid misfitting of the gaussian, take directly std of values\n",
    "        ## in the histogram as the uncertainty\n",
    "        inverse_means[j,k] = np.mean(inverse_toys[j,k,:])\n",
    "        inverse_errors[j,k] = np.std(inverse_toys[j,k,:])\n",
    "\n",
    "print(f\"Erros for the inverse matrix:\\n{inverse_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Separate $t$- and $s$-channel contributions\n",
    "\n",
    "Only Feynman diagrams contributing to the production of $Z$ boson are to be considered for the measurements. The **electron** Monte Carlo sample incorporate contributions from $t$- and $s$-channels.\n",
    "* Select/correct contributions producing $Z$ boson decays. (Hint: Which role does the $\\cos(\\theta)$ distribution play in separating $t$- and $s$-channels?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Measurement of the total production cross sections\n",
    "\n",
    "For **each** of the seven centre-of-mass energies:\n",
    "* Determine the number of events in the handronic channel *and* in the three leptonic channels\n",
    "* Substract the background and correct for selection efficiencies accordingly\n",
    "* Then, calculate the differnetial cross sections for the hadronic *and* the leptnic channels\n",
    "* Add the radiation corrections from The table given below. **Don't forget to take the uncertainties (errors) into account!**\n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Correction hadronic channel    \\[nb\\] |  Correction leptonic channel   \\[nb\\]|\n",
    "| --- | --- | --- |\n",
    "| 88.47 | +2.0  | +0.09 |\n",
    "| 89.46 | +4.3  | +0.20 |\n",
    "| 90.22 | +7.7  | +0.36 |\n",
    "| 91.22 | +10.8 | +0.52 |\n",
    "| 91.97 | +4.7  | +0.22 |\n",
    "| 92.96 | -0.2  | -0.01 |\n",
    "| 93.76 | -1.6  | -0.08 |\n",
    "\n",
    "Feel free to access these values using the dictionary 'xs_corrections' given below.\n",
    "* Once the total cross section for all four decay channels at all seven energies have been measured, fit a **Breit-Wigner distribution** to measure the $Z$ boson mass ($m_Z$) and the resonance width ($\\Gamma_Z$) and the peak cross section s of the resonance for the hadronic and the leptonic channels. Again, **propagate the uncertainties carefully**.\n",
    "* Compare your results to the OPAL cross section s and the theoretical predictions. How many degrees of freedom does the fit have? How can you udge if the model is compatible with the measured data? Calculate the  **confidence levels**.\n",
    "* Calculate the partial widths for all channels from the measured cross sections on the peak. Which is the best partial width to start with? Compare them with the theoretical predictions and the values that you have calculated in the beginning.\n",
    "* Determine from your results the **number of generations of light neutrinos**. Which assumptions are necessary?\n",
    "* Discuss in detail the systematic uncertainties in the whole procedure of the analysis. Which assumptions were necessary?\n",
    "\n",
    "These are some **references** that might be interesting to look up:\n",
    "* Particle Data Book: https://pdg.lbl.gov/2020/download/Prog.Theor.Exp.Phys.2020.083C01.pdf\n",
    "** Resonances: https://pdg.lbl.gov/2017/reviews/rpp2017-rev-resonances.pdf\n",
    "* Precision Electroweak Measurements on the Z Resonance (Combination LEP): https://arxiv.org/abs/hep-ex/0509008\n",
    "* Measurement of the $Z^0$ mass and width with the OPAL detector at LEP: https://doi.org/10.1016/0370-2693(89)90705-3\n",
    "* Measurement of the $Z^0$ line shape parameters and the electroweak couplings of charged leptons: https://inspirehep.net/literature/315269\n",
    "* The OPAL Collaboration, *Precise Determination of the $Z$ Resonance Parameters at LEP: \"Zedometry\"*: https://arxiv.org/abs/hep-ex/0012018\n",
    "* Fitting a Breit-Wigner curve using uproot: https://masonproffitt.github.io/uproot-tutorial/07-fitting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_corrections = { 'energy' : [ 88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                      'hadronic' : [2.0, 4.3, 7.7, 10.8, 4.7, -0.2, -1.6],\n",
    "                      'leptonic' : [0.09, 0.20, 0.36, 0.52, 0.22, -0.01, -0.08]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Forward-backward asymmetry and $\\sin^2(\\theta_\\text{W})$ in muon final states\n",
    "\n",
    "* Using the **muon channel only**, measure the forward-backward asymmetry $\\mathcal{A}_\\text{FB}$ using OPAL data and muon Monte Carlo events. Take into account the radiation corrections given below. \n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Radiation correction [-]|  \n",
    "| --- | --- | \n",
    "| 88.47 | 0.021512  | \n",
    "| 89.46 | 0.019262  | \n",
    "| 90.22 | 0.016713  | \n",
    "| 91.22 | 0.018293  | \n",
    "| 91.97 | 0.030286  | \n",
    "| 92.96 | 0.062196  | \n",
    "| 93.76 | 0.093850  | \n",
    "\n",
    "Feel free to use the dictionary 'radiation_corrections' given below.\n",
    "\n",
    "* Measure the **Weinberg angle** as $\\sin^2(\\theta_\\text{W})$. Compare the measurement with the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiation_corrections = { 'energy' : [ 88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                          'correction' : [0.021512, 0.019262, 0.016713, 0.018293, 0.030286, 0.062196, 0.093850]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Tests on lepton universalityÂ¶\n",
    "\n",
    "* Test the lepton universality from the total cross sectinos on the peak for $Z\\to e^+ e^-$, $Z\\to \\mu^+ \\mu^-$ and $Z\\to \\tau^+ \\tau^-$ events. What is the ratio of the total cross section of the hadronic channel to the leptonic channels on the peak? Compare with the ratios obtained from the branching rations and discuss possible differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
